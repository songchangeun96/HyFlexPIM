{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7bd2c77",
   "metadata": {},
   "source": [
    "# HyFlexPIM LLaMA3 Notebook\n",
    "\n",
    "This notebook uses functions and classes from `hyflex_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM, DataCollatorForLanguageModeling, get_scheduler, LlamaTokenizerFast, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import chain\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import copy\n",
    "from hyflex_utils import *  # Import all necessary functions/classes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" \n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677725f",
   "metadata": {},
   "source": [
    "Step 1. Load the models (Run all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c82f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "#     device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "datasets = load_dataset(\"ptb_text_only\", \"penn_treebank\")\n",
    "dataset_ = \"ptb\"\n",
    "\n",
    "accelerator = Accelerator() \n",
    "\n",
    "column_names = datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=CausalDataCollator(tokenizer), batch_size=1)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=True, collate_fn=CausalDataCollator(tokenizer), batch_size=1)\n",
    "model, train_dataloader, eval_dataloader = accelerator.prepare(model, train_dataloader, eval_dataloader)\n",
    "\n",
    "model.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376500bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference (You can skip this part)\n",
    "\n",
    "evaluate_model(model, tokenizer, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efdcae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Change this if you are using a different task\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1, \n",
    ")\n",
    "\n",
    "# Wrap your model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer], 'lr': 5e-6}]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56256f",
   "metadata": {},
   "source": [
    "Step 2. Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training original model (We are going to apply SVD on the best trained model)\n",
    "\n",
    "train_model(model, tokenizer, optimizer, train_dataloader, eval_dataloader, accelerator, epochs=1, grad_accum_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb06071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the model (Not necessary, but recommend to store your best model)\n",
    "\n",
    "save_dir = f'./model_{dataset_}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), f'./model_{dataset_}/finetuned_best')\n",
    "print(\"Model weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ca0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must run this\n",
    "\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ddb76",
   "metadata": {},
   "source": [
    "Step 3. SVD decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace linear layer with SVD decomposed & traniner layer\n",
    "\n",
    "replace_linear_layer_llama(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774697fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this after replace_linear_layer\n",
    "\n",
    "model, train_dataloader, eval_dataloader = accelerator.prepare(model, train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88932225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference (To check accuarcy degradation after svd, not necessary)\n",
    "\n",
    "evaluate_model(model, tokenizer, eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a5cee",
   "metadata": {},
   "source": [
    "Step 4. Fine tuning & Gradient redistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  You can change lr\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "#  Trainable Parameters \n",
    "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "print(f\" Trainable Parameters Count: {len(trainable_params)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9587098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVD-ed model (w/ gradient redistribution)\n",
    "\n",
    "train_model_llama_gradient_saving(model, tokenizer, optimizer, train_dataloader, eval_dataloader, accelerator, epochs=1, grad_accum_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model!\n",
    "\n",
    "torch.save(model.state_dict(), f'./model_{dataset_}/finetuned_best_after_svd')\n",
    "\n",
    "print(\"Model weights after svd saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcf17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load model (optional)\n",
    "\n",
    "# model.load_state_dict(torch.load(f'./model_{dataset_}/finetuned_best_after_svd'))\n",
    "model.load_state_dict(torch.load(f'../../ISCA_SVD/model_ptb/finetuned_best_after_svd_2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f24057",
   "metadata": {},
   "source": [
    "Step 5. Simulation loop for all SLC cases, and generating graph\n",
    " : You might change list of thresholds (which is SLC rate);\n",
    " e.g., thresholds = [0, 20] --> Simulation for 0% of SLC rate, and 20% of SLC rate --> generating plot figures! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c356da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.025\n",
    "thresholds = [0, 5, 10, 30, 40, 50, 100]\n",
    "model_path = f'./model_{task}/finetuned_best_after_svd'\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for th in thresholds:\n",
    "    print(f\"\\n==== [Threshold: {th}%] ====\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    load_gradients(model)\n",
    "    model = apply_noise_to_llama(model, std, th)\n",
    "    loss, ppl = evaluate_llama(model, eval_dataloader, tokenizer)\n",
    "    accuracies.append(1 / ppl) \n",
    "    print(f\"[Threshold={th}%] Perplexity: {ppl:.2f}\")\n",
    "\n",
    "\n",
    "accuracy_percent = [a * 100 for a in accuracies]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar([str(t) + \"%\" for t in thresholds], accuracy_percent, color='skyblue')\n",
    "plt.title(\"Noise Injection Threshold vs Inverse Perplexity (LLaMA3)\")\n",
    "plt.xlabel(\"Noise Injection Threshold (%)\")\n",
    "plt.ylabel(\"1 / Perplexity Ã— 100\")\n",
    "plt.ylim(0, max(accuracy_percent) * 1.1)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc97424",
   "metadata": {},
   "source": [
    "Step 5-. Noise Injection simulation with single threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.025  # Default is 0.025, but you can change based on your error rate\n",
    "th = 25  # Default is 25 which means 25% of weights will be stored in SLC\n",
    "\n",
    "load_gradients(model) # load gradient\n",
    "\n",
    "model = apply_noise_to_llama(model, std, th) # Replace weights with noise injected weights. # Change clipping value in function get_clipping_value (inside hyflex_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with noise injected model\n",
    "\n",
    "evaluate_model(model, tokenizer, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22216ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AE_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
