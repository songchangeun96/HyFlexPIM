{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7bd2c77",
   "metadata": {},
   "source": [
    "# HyFlexPIM BERT Notebook\n",
    "\n",
    "This notebook uses functions and classes from `hyflex_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM, DataCollatorForLanguageModeling, get_scheduler, LlamaTokenizerFast, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import Value\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import chain\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import copy\n",
    "from hyflex_utils import *  # Import all necessary functions/classes\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" \n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677725f",
   "metadata": {},
   "source": [
    "Step 1. Load the models (Run all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f83cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Configuration ==========\n",
    "model_ = \"bert-large-uncased\"\n",
    "# model_ = \"bert-base-uncased\" \n",
    "task = \"mrpc\"  # Change to \"sst2\", \"cola\", \"qnli\", \"qqp\", \"mrpc\", \"stsb\", \"rte\"\n",
    "\n",
    "cfg = {\n",
    "    \"seed\": 42,\n",
    "    \"batch_size\": 16,\n",
    "    \"lr\": 2e-5,\n",
    "    \"n_epochs\": 3,\n",
    "    \"warmup\": 0.1,\n",
    "    \"save_steps\": 100,\n",
    "}\n",
    "\n",
    "torch.manual_seed(cfg[\"seed\"])\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# ========== Tokenizer & Model ==========\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_)\n",
    "\n",
    "# Task-specific num_labels\n",
    "task_to_num_labels = {\n",
    "    \"sst2\": 2,\n",
    "    \"cola\": 2,\n",
    "    \"qnli\": 2,\n",
    "    \"qqp\": 2,\n",
    "    \"mrpc\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"stsb\": 1,  # Regression\n",
    "}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_,\n",
    "    num_labels=task_to_num_labels[task],\n",
    "    problem_type=\"regression\" if task == \"stsb\" else \"single_label_classification\"\n",
    ")\n",
    "\n",
    "# ========== Preprocessing Functions ==========\n",
    "def sst2_preprocess(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def cola_preprocess(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def qnli_preprocess(examples):\n",
    "    return tokenizer(examples[\"question\"], examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def qqp_preprocess(examples):\n",
    "    return tokenizer(examples[\"question1\"], examples[\"question2\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def mrpc_preprocess(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def stsb_preprocess(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def rte_preprocess(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "preprocess_functions = {\n",
    "    \"sst2\": sst2_preprocess,\n",
    "    \"cola\": cola_preprocess,\n",
    "    \"qnli\": qnli_preprocess,\n",
    "    \"qqp\": qqp_preprocess,\n",
    "    \"mrpc\": mrpc_preprocess,\n",
    "    \"stsb\": stsb_preprocess,\n",
    "    \"rte\": rte_preprocess,\n",
    "}\n",
    "\n",
    "# ========== Load and Preprocess Dataset ==========\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"glue\", task)\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(preprocess_functions[task], batched=True)\n",
    "val_dataset = dataset[\"validation\"].map(preprocess_functions[task], batched=True)\n",
    "test_dataset = dataset.get(\"test\", val_dataset).map(preprocess_functions[task], batched=True)  # fallback if no test set\n",
    "\n",
    "\n",
    "if task == \"stsb\":\n",
    "    train_dataset = train_dataset.cast_column(\"label\", Value(\"float32\"))\n",
    "    val_dataset = val_dataset.cast_column(\"label\", Value(\"float32\"))\n",
    "    test_dataset = test_dataset.cast_column(\"label\", Value(\"float32\"))\n",
    "else:\n",
    "    train_dataset = train_dataset.cast_column(\"label\", Value(\"int64\"))\n",
    "    val_dataset = val_dataset.cast_column(\"label\", Value(\"int64\"))\n",
    "    test_dataset = test_dataset.cast_column(\"label\", Value(\"int64\"))\n",
    "\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# ========== DataLoaders ==========\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=cfg[\"batch_size\"])\n",
    "eval_dataloader = DataLoader(val_dataset, shuffle=False, collate_fn=data_collator, batch_size=cfg[\"batch_size\"])\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, collate_fn=data_collator, batch_size=cfg[\"batch_size\"])\n",
    "\n",
    "# ========== Prepare with Accelerator ==========\n",
    "model, train_dataloader, eval_dataloader = accelerator.prepare(model, train_dataloader, eval_dataloader)\n",
    "model.to(torch.float32)\n",
    "\n",
    "print(f\"âœ” Loaded task: {task}, train size: {len(train_dataset)}, val size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376500bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference (You can skip this part)\n",
    "\n",
    "evaluate_model_bert(model, tokenizer, eval_dataloader, task=task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56256f",
   "metadata": {},
   "source": [
    "Step 2. Train the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training original model (We are going to apply SVD on the best trained model)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5) # you can change the lr, 5e-5 is default\n",
    "train_model_bert(model, tokenizer, optimizer, train_dataloader, eval_dataloader, accelerator, epochs=3, grad_accum_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb06071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the model (Not necessary, but recommend to store your best model)\n",
    "\n",
    "save_dir = f'./model_{task}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), f'./model_{task}/finetuned_best')\n",
    "print(\"Model weights saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2339d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load the model (Optional)\n",
    "# model.load_state_dict(torch.load(f'./model_{task}/finetuned_best'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ddb76",
   "metadata": {},
   "source": [
    "Step 3. SVD decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace linear layer with SVD decomposed & traniner layer\n",
    "\n",
    "replace_linear_layer_bert(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88932225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference (To check accuarcy degradation after svd, not necessary)\n",
    "\n",
    "evaluate_model_bert(model, tokenizer, eval_dataloader, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a5cee",
   "metadata": {},
   "source": [
    "Step 4. Fine tuning & Gradient redistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  You can change lr\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "model, train_dataloader, eval_dataloader = accelerator.prepare(model, train_dataloader, eval_dataloader)\n",
    "\n",
    "\n",
    "#  Trainable Parameters \n",
    "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "print(f\" Trainable Parameters Count: {len(trainable_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9587098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training SVD-ed model (w/ gradient redistribution)\n",
    "\n",
    "train_model_bert_gradient_saving(model, tokenizer, optimizer, train_dataloader, eval_dataloader, accelerator, epochs=3, grad_accum_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model!\n",
    "\n",
    "torch.save(model.state_dict(), f'./model_{task}/finetuned_best_after_svd')\n",
    "\n",
    "print(\"Model weights after svd saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcf17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load model (optional)\n",
    "\n",
    "# model.load_state_dict(torch.load(f'./model_{task}/finetuned_best_after_svd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc97424",
   "metadata": {},
   "source": [
    "Step 5. Noise Injection simulation\n",
    " : You might change list of thresholds (which is SLC rate);\n",
    " e.g., thresholds = [0, 20] --> Simulation for 0% of SLC rate, and 20% of SLC rate --> generating plot figures! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c16b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with noise injected model\n",
    "\n",
    "std = 0.025\n",
    "thresholds = [0, 5, 10, 30, 40, 50, 100]\n",
    "model_path = f'./model_{task}/finetuned_best_after_svd'\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for th in thresholds:\n",
    "    print(f\"\\n==== [Threshold: {th}%] ====\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    load_gradients_bert(model)\n",
    "    model = apply_noise_to_bert(model, std, th)\n",
    "    acc = evaluate_model_bert(model, tokenizer, eval_dataloader, task=task)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"[Threshold={th}%] Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "\n",
    "accuracy_percent = [a * 100 for a in accuracies]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar([str(t) + \"%\" for t in thresholds], accuracy_percent, color='skyblue')\n",
    "plt.title(\"Noise Injection Threshold vs Validation Accuracy (BERT)\")\n",
    "plt.xlabel(\"Noise Injection Threshold (%)\")\n",
    "plt.ylabel(\"Validation Accuracy (%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c7300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AE_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
